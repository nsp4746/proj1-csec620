{
 "cells": [
  {
   "cell_type": "code",
   "id": "e59c0b394860a347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T23:16:08.167999Z",
     "start_time": "2024-09-25T23:14:21.607770Z"
    }
   },
   "source": [
    "# Nikhil Patil\n",
    "# CSEC 620\n",
    "# Project 1 New Model\n",
    "'''\n",
    "this model is far more efficient than my previous one. \n",
    "It uses the same dataset, but it uses the clean_df function from the dataprep library to clean the dataset as well all 77 features rather than just 4.  \n",
    "The larger sequential keras model versus functional keras model aids in efficiency as well. \n",
    "\n",
    "'''\n",
    "\n",
    "# Import the necessary libraries\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataprep.clean import clean_df\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # ignore warnings\n",
    "\n",
    "dataset_csv_path = 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv' # the dataset used for this model. \n",
    "\n",
    "# Load and clean the dataset \n",
    "df = pd.read_csv(dataset_csv_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "df.columns = df.columns.str.replace(\"/\", \"\", regex=False) \n",
    "df = clean_df(df,data_type_detection = \"none\", standardize_missing_values = \"remove\") \n",
    "'''\n",
    "The clean_df function is used to clean the dataset. The data_type_detection parameter is set to \"none\" because of the way that clean_df returns the dataframe.\n",
    "standardize_missing_values is set to \"remove\" to remove rows with missing values.\n",
    "'''\n",
    "\n",
    "# create list of features\n",
    "features = []\n",
    "for column in df.columns:\n",
    "    features.append(column)\n",
    "\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True) # replace infinite values with Not a Number (NaN)\n",
    "df.fillna(0, inplace=True) # fill values with 0 \n",
    "\n",
    "df = df[features]\n",
    "df_features = df.drop('label', axis=1) # drop the label column\n",
    "df_labels = df['label'] # get the label column\n",
    "\n",
    "\n",
    "# Begin to fit the model\n",
    "scaler = StandardScaler() \n",
    "transformed_features = scaler.fit_transform(df_features)\n",
    "\n",
    "# Encode the Label column\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "labels_encoded = encoder.fit_transform(df_labels.values.reshape(-1,1))\n",
    "\n",
    "# Use train_test_split from scikit-learn to split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_features, labels_encoded, train_size= .8, test_size=0.2)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(labels_encoded.shape[1], activation='sigmoid')  # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "start_time = time.perf_counter()\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"Loss: {loss: .8f}, Accuracy: {accuracy: .12f}\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Headers Cleaning Report:\n",
      "\t77 values cleaned (97.47%)\n",
      "Number of Entries Cleaning Report:\n",
      "\t4 entries dropped (0.0%)\n",
      "Downcast Memory Report:\n",
      "\tMemory reducted from 174241739 to 98844245. New size: (56.73%)\n",
      "Epoch 1/3\n",
      "5644/5644 [==============================] - 27s 5ms/step - loss: 0.0168 - accuracy: 0.9964\n",
      "Epoch 2/3\n",
      "5644/5644 [==============================] - 25s 4ms/step - loss: 0.0101 - accuracy: 0.9986\n",
      "Epoch 3/3\n",
      "5644/5644 [==============================] - 25s 5ms/step - loss: 0.0071 - accuracy: 0.9985\n",
      "1411/1411 [==============================] - 5s 3ms/step - loss: 0.0042 - accuracy: 0.9991\n",
      "Loss:  0.004238, Accuracy:  1.00\n",
      "Time taken:  82.63 seconds\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
